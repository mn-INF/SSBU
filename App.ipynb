{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7948ddb-59e7-4778-80d9-1769663f0559",
   "metadata": {},
   "source": [
    "<h1> App Station</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f3b44e-4f53-4fde-bd4f-d8520e397cf2",
   "metadata": {},
   "source": [
    "This notebook is built to call the \"Prediction.py\" file and allows us to run the model we built on our local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f18fe3d-b15b-4638-9b99-ddf12afacfaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import statements\n",
    "\n",
    "from flask import Flask, request, jsonify, render_template\n",
    "from flask_restful import Resource, Api\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pickle\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66545eb5-16af-49e7-87f9-05218cf6cad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Flask API call\n",
    "\n",
    "app = Flask(__name__, template_folder='templates')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a87f223-4f04-4e92-aa2a-539b70b936d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LemmaTokenizer(object): \n",
    "#we need this class here so that the pipeline can pull the class from somewhere, the model can't save the class we made in the pkl file\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, articles):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(articles)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a18dafab-7186-438a-87e2-281fc448e541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained model (ensure 'model.pkl' is in the same directory or adjust the path)\n",
    "try:\n",
    "    with open('model.pkl', 'rb') as f:\n",
    "        model = pickle.load(f)\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: model.pkl not found. Please ensure your trained model file is present.\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e34ba5c-5054-4aa3-a3e3-a3affe4f79bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/')\n",
    "def home():\n",
    "    \"\"\"Renders the main page (e.g., index.html) with a form for input.\"\"\"\n",
    "    return render_template('index.html')\n",
    "\n",
    "@app.route('/predict', methods=['GET', 'POST'])\n",
    "def predict():\n",
    "    data = request.get_json()\n",
    "    # Convert input data to a format your model expects (e.g., DataFrame)\n",
    "    # This step depends entirely on your model's specific input requirements\n",
    "    input_data = pd.DataFrame(data)\n",
    "\n",
    "    # Make a prediction\n",
    "    prediction = model.predict(input_data)\n",
    "\n",
    "    if prediction[0] == 1:\n",
    "        # Return the result to the user\n",
    "        return {'predict': 'Will make top 8'}\n",
    "            \n",
    "    elif prediction[0] == 0:\n",
    "        # Return the result to the user\n",
    "        return {'predict': 'Will not make top 8'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "814ed035-74d1-412b-b775-20fe1d8092e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:5000\n",
      " * Running on http://192.168.1.6:5000\n",
      "Press CTRL+C to quit\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:402: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "127.0.0.1 - - [31/Dec/2025 23:15:24] \"POST /predict HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [31/Dec/2025 23:15:47] \"POST /predict HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [31/Dec/2025 23:15:49] \"POST /predict HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "#initializing the API call we need\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run the app\n",
    "    # Debug mode is useful for development\n",
    "    app.run(host='0.0.0.0')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
